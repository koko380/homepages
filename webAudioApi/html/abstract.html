<html>
  <head lang="ja">
    <meta charset="UTF-8">
    <title>概要</title>
		<link href="./css/mod.css" rel="stylesheet" type="text/css">
	<head>

	<body>
	<h3>AudioContext</h3>
	<p>
		Web Audio APIを利用するためには、AudioContextオブジェクトのコンストラクタを呼び出して、インスタンスを生成する必要があります。<br>
		AudioContextインスタンスを生成することで、Web Audio APIが定義するプロパティやメソッドにアクセス可能になります。
	</p>

	<div class="imgCenter">
	<figure>
		<img src="https://www.w3.org/TR/webaudio/images/modular-routing2.png">
		<figcaption>AudioContextとノード(<a href="https://www.w3.org/TR/webaudio/#ModularRouting" target="_blank" rel="noopener noreferrer">引用元</a>)</figcaption>
	</figure>
	</div>

	<h3>サウンドを生成するためには</h3>
	<p>
		サウンドを生成する時に、AudioContextインスタンスで最も重要なのは以下の2点です。
	</p>
	<ul>
		<li>サウンド入力のためのノードとそれを生成するメソッド</li>
		<li>サウンド出力のためのdestinationプロパティ</li>
	</ul>
	<p>
		初出の用語としてノードが出てきました。このノードというものは、要素と要素が線によって結ばれたデータ構造のことです。<br>
		Web Audio APIでは<a href="https://www.g200kg.com/jp/docs/webaudio/paramlist.html" target="_blank" rel="noopener noreferrer">様々な種類のノード</a>が用意されています。これらを必要に応じて作成し、接続していくことで様々な音を生成したり、加工したりできるようになります。<br><br>
		サウンドの出力点の役割を担うのはAudioDestinationNodeインスタンスにあるdestinationプロパティのみです。対して、サウンドの入力点の役割を担うノードは複数あります。表1にそれらを簡単にまとめました。
	</p>
	<table border="1">
		<caption>表1.サウンド入力のためのノードと役割</caption>
		<tr>
			<th>ノード</th>
			<th>役割</th>
		</tr>
		<tr>
      <td>OscillatorNode</td>
      <td>指定された周波数と波形の連続した信号の出力</td>
    </tr>
		<tr>
      <td>MediaElementAudioSourceNode</td>
      <td>楽曲データの再生</td>
    </tr>
		<tr>
			<td>AudioBufferSourceNode</td>
			<td>楽器音などのワンショットサンプルの再生</td>
		</tr>
		<tr>
      <td>MediaStreamAudioSourceNode</td>
      <td>WebRTCによって取得したデバイスからの入力を扱う</td>
    </tr>
		<tr>
      <td>AudioWorkletNode</td>
      <td>生の音声データを自由に操作可能</td>
    </tr>
		<tr>
      <td>ScriptProcessorNode</td>
      <td>廃止予定(AudioWorkletNodeが後継となります)</td>
    </tr>
	</table>
	</body>
</html>
